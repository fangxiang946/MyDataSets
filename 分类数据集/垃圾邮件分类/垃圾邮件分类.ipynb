{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取数据\n",
    "def get_data():\n",
    "    '''\n",
    "    获取数据\n",
    "    :return: 文本数据，对应的labels\n",
    "    '''\n",
    "    with open(\"ham_data.txt\", encoding=\"utf8\") as ham_f, open(\"spam_data.txt\", encoding=\"utf8\") as spam_f:\n",
    "        # 正常的邮件数据\n",
    "        ham_data = ham_f.readlines()\n",
    "        # 垃圾邮件数据\n",
    "        spam_data = spam_f.readlines()\n",
    "        \n",
    "        # 正常数据标记为1\n",
    "        ham_label = np.ones(len(ham_data)).tolist()\n",
    "        # 垃圾邮件数据标记为0\n",
    "        spam_label = np.zeros(len(spam_data)).tolist()\n",
    "        \n",
    "        # 数据集集合\n",
    "        corpus = ham_data + spam_data\n",
    "         \n",
    "        # 标记数据集合\n",
    "        labels = ham_label + spam_label\n",
    "\n",
    "    return corpus, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据分割\n",
    "def prepare_datasets(corpus, labels, test_data_proportion=0.3):\n",
    "    '''\n",
    "    :param corpus: 文本数据\n",
    "    :param labels: label数据\n",
    "    :param test_data_proportion:测试数据占比 \n",
    "    :return: 训练数据,测试数据，训练label,测试label\n",
    "    '''\n",
    "    train_X, test_X, train_Y, test_Y = train_test_split(corpus, labels,\n",
    "                                                        test_size=test_data_proportion, random_state=42)\n",
    "    return train_X, test_X, train_Y, test_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 移除空的数据\n",
    "def remove_empty_docs(corpus, labels):\n",
    "    filtered_corpus = []\n",
    "    filtered_labels = []\n",
    "    for doc, label in zip(corpus, labels):\n",
    "        if doc.strip():\n",
    "            filtered_corpus.append(doc)\n",
    "            filtered_labels.append(label)\n",
    "\n",
    "    return filtered_corpus, filtered_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "总的数据量: 10001\n",
      "样本之一: 北京售票员可厉害，嘿嘿，有专座的，会直接拉着脖子指着鼻子让上面的人站起来让 座的，呵呵，比较赞。。。 杭州就是很少有人给让座，除非司机要求乘客那样做。 五一去杭州一个景点玩，车上有两个不到一岁的小孩，就是没有人给让座，没办法家长只能在车上把小孩的推车打开让孩子坐进去，但是孩子还是闹，只能抱着，景点离市区很远，车上很颠，最后家长坐在地上抱孩子，就是没有一个人给让座，要是在北京，一上车就有人让座了\n",
      "\n",
      "样本的label: 1.0\n",
      "实际类型: 正常邮件 垃圾邮件\n"
     ]
    }
   ],
   "source": [
    "corpus, labels = get_data()  # 获取数据集\n",
    "\n",
    "print(\"总的数据量:\", len(labels))\n",
    "\n",
    "corpus, labels = remove_empty_docs(corpus, labels)\n",
    "\n",
    "print('样本之一:', corpus[10])\n",
    "print('样本的label:', labels[10])\n",
    "label_name_map = [\"垃圾邮件\", \"正常邮件\"]\n",
    "print('实际类型:', label_name_map[int(labels[10])], label_name_map[int(labels[5900])])\n",
    "\n",
    "# 对数据进行划分\n",
    "train_corpus, test_corpus, train_labels, test_labels = prepare_datasets(corpus,\n",
    "                                                                        labels,\n",
    "                                                                        test_data_proportion=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 归一化函数\n",
    "\n",
    "import re\n",
    "import string\n",
    "import jieba\n",
    "\n",
    "# 加载停用词\n",
    "with open(\"stop_words.utf8\", encoding=\"utf8\") as f:\n",
    "    stopword_list = f.readlines()\n",
    "\n",
    "# 分词\n",
    "def tokenize_text(text):\n",
    "    tokens = jieba.cut(text)\n",
    "    tokens = [token.strip() for token in tokens]\n",
    "    return tokens\n",
    "\n",
    "\n",
    "# 去掉特殊符号\n",
    "def remove_special_characters(text):\n",
    "    tokens = tokenize_text(text)\n",
    "    pattern = re.compile('[{}]'.format(re.escape(string.punctuation)))\n",
    "    filtered_tokens = filter(None, [pattern.sub('', token) for token in tokens])\n",
    "    filtered_text = ' '.join(filtered_tokens)\n",
    "    return filtered_text\n",
    "\n",
    "# 取出停用词\n",
    "def remove_stopwords(text):\n",
    "    tokens = tokenize_text(text)\n",
    "    filtered_tokens = [token for token in tokens if token not in stopword_list]\n",
    "    filtered_text = ''.join(filtered_tokens)\n",
    "    return filtered_text\n",
    "\n",
    "# 标准化数据集\n",
    "def normalize_corpus(corpus, tokenize=False):\n",
    "    # 声明一个变量用来存储标准化后的数据\n",
    "    normalized_corpus = []\n",
    "    for text in corpus:\n",
    "            # 去掉特殊符号\n",
    "        text = remove_special_characters(text)\n",
    "            # 取出停用词\n",
    "        text = remove_stopwords(text)\n",
    "        normalized_corpus.append(text)\n",
    "        if tokenize:\n",
    "            text = tokenize_text(text)\n",
    "            normalized_corpus.append(text)\n",
    "\n",
    "    return normalized_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /var/folders/yk/j1gndz3j7zzd7vxlzs6wj1q00000gn/T/jieba.cache\n",
      "Loading model cost 0.985 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 进行归一化\n",
    "norm_train_corpus = normalize_corpus(train_corpus)\n",
    "norm_test_corpus = normalize_corpus(test_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "基于tfidf的贝叶斯模型\n",
      "准确率: 0.79\n",
      "精度: 0.85\n",
      "召回率: 0.79\n",
      "F1得分: 0.78\n"
     ]
    }
   ],
   "source": [
    "# 特征提取\n",
    "\n",
    "import gensim\n",
    "import jieba\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def tfidf_extractor(corpus, ngram_range=(1, 1)):\n",
    "    vectorizer = TfidfVectorizer(min_df=1,\n",
    "                                 norm='l2',\n",
    "                                 smooth_idf=True,\n",
    "                                 use_idf=True,\n",
    "                                 ngram_range=ngram_range)\n",
    "    features = vectorizer.fit_transform(corpus)\n",
    "    return vectorizer, features\n",
    "\n",
    "# tfidf 特征    标准化后的数据送入函数进行提取\n",
    "tfidf_vectorizer, tfidf_train_features = tfidf_extractor(norm_train_corpus)\n",
    "tfidf_test_features = tfidf_vectorizer.transform(norm_test_corpus)\n",
    "\n",
    "\n",
    "# 导入贝叶斯模型\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "mnb = MultinomialNB()\n",
    "\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "# 模型性能预测函数\n",
    "def get_metrics(true_labels, predicted_labels):\n",
    "    print('准确率:', np.round(\n",
    "        metrics.accuracy_score(true_labels,\n",
    "                               predicted_labels),\n",
    "        2))\n",
    "    print('精度:', np.round(\n",
    "        metrics.precision_score(true_labels,\n",
    "                                predicted_labels,\n",
    "                                average='weighted'),\n",
    "        2))\n",
    "    print('召回率:', np.round(\n",
    "        metrics.recall_score(true_labels,\n",
    "                             predicted_labels,\n",
    "                             average='weighted'),\n",
    "        2))\n",
    "    print('F1得分:', np.round(\n",
    "        metrics.f1_score(true_labels,\n",
    "                         predicted_labels,\n",
    "                         average='weighted'),\n",
    "        2))\n",
    "\n",
    "\n",
    "# 模型调用函数，这样做的好处是，可以自己人选模型\n",
    "def train_predict_evaluate_model(classifier,\n",
    "                                 train_features, train_labels,\n",
    "                                 test_features, test_labels):\n",
    "    # 模型构建\n",
    "    classifier.fit(train_features, train_labels)\n",
    "    # 使用哪个模型做预测\n",
    "    predictions = classifier.predict(test_features)\n",
    "    # 评估模型预测性能\n",
    "    get_metrics(true_labels=test_labels,\n",
    "                predicted_labels=predictions)\n",
    "    return predictions\n",
    "\n",
    "\n",
    "# 基于tfidf的多项式朴素贝叶斯模型\n",
    "print(\"基于tfidf的贝叶斯模型\")\n",
    "mnb_tfidf_predictions = train_predict_evaluate_model(classifier=mnb,\n",
    "                                                     train_features=tfidf_train_features,\n",
    "                                                     train_labels=train_labels,\n",
    "                                                     test_features=tfidf_test_features,\n",
    "                                                     test_labels=test_labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
